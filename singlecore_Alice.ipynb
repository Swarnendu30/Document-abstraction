{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee472c17-f490-4579-9e6d-5faf45f6227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfplumber nltk sentence-transformers qdrant-client\n",
    "\n",
    "import pdfplumber\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb425608-fb20-4a1a-8607-fff62dd63da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"document_chunks\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "MAX_CHUNK_WORDS = 200\n",
    "\n",
    "# Initialize Qdrant\n",
    "client = QdrantClient(url=QDRANT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688353b7-1981-4ffd-89d4-8bf8ed6990da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF with page numbers\n",
    "def extract_text_with_pages(pdf_path):\n",
    "    text_with_pages = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                text_with_pages.append((page_num, text))\n",
    "    return text_with_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f150a094-835a-42f8-bbad-72b6bc4071c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic dynamic chunking\n",
    "def semantic_chunking(text_with_pages, threshold=0.75):\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    chunks = []\n",
    "\n",
    "    for page_number, text in text_with_pages:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        sentence_embeddings = model.encode(sentences)\n",
    "        current_chunk = []\n",
    "        current_chunk_words = 0\n",
    "\n",
    "        for i, (sentence, embedding) in enumerate(zip(sentences, sentence_embeddings)):\n",
    "            if not current_chunk:\n",
    "                current_chunk.append(sentence)\n",
    "                current_chunk_words = len(sentence.split())\n",
    "                continue\n",
    "\n",
    "            sim = np.dot(embedding, model.encode([' '.join(current_chunk)])[0]) / (\n",
    "                np.linalg.norm(embedding) * np.linalg.norm(model.encode([' '.join(current_chunk)])[0])\n",
    "            )\n",
    "\n",
    "            if sim > threshold and current_chunk_words + len(sentence.split()) <= MAX_CHUNK_WORDS:\n",
    "                current_chunk.append(sentence)\n",
    "                current_chunk_words += len(sentence.split())\n",
    "            else:\n",
    "                chunks.append({\n",
    "                    \"text\": \" \".join(current_chunk),\n",
    "                    \"page_number\": page_number\n",
    "                })\n",
    "                current_chunk = [sentence]\n",
    "                current_chunk_words = len(sentence.split())\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"text\": \" \".join(current_chunk),\n",
    "                \"page_number\": page_number\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528ed4fa-0c41-498e-b061-092f9e4dad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "def generate_embeddings(chunks):\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = model.encode(texts, normalize_embeddings=True)\n",
    "    return embeddings\n",
    "\n",
    "# Create collection if not exists\n",
    "def create_collection():\n",
    "    if COLLECTION_NAME not in [c.name for c in client.get_collections().collections]:\n",
    "        client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=rest.VectorParams(size=384, distance=rest.Distance.COSINE)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f55a1e1-1eff-410f-84d3-3e8c02878f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store chunks and metadata in Qdrant\n",
    "def store_chunks(chunks, embeddings):\n",
    "    points = []\n",
    "    payloads = []\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        points.append(rest.PointStruct(\n",
    "            id=chunk_id,\n",
    "            vector=embedding.tolist()\n",
    "        ))\n",
    "        payloads.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk['text'],\n",
    "            \"page_number\": chunk['page_number']\n",
    "        })\n",
    "\n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "\n",
    "    # Set payloads separately (for compatibility)\n",
    "    for point, payload in zip(points, payloads):\n",
    "        client.set_payload(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            payload=payload,\n",
    "            points=[point.id]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e3cd63-bf62-4800-baf9-97b0603903c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vector search and keyword overlap\n",
    "def search(query, top_k=5):\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    query_embedding = model.encode([query], normalize_embeddings=True)[0]\n",
    "    search_results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for result in search_results:\n",
    "        payload = result.payload\n",
    "        chunk_text = payload[\"text\"]\n",
    "        keywords_query = set(re.findall(r'\\w+', query.lower()))\n",
    "        keywords_chunk = set(re.findall(r'\\w+', chunk_text.lower()))\n",
    "        keyword_overlap = len(keywords_query & keywords_chunk) / max(len(keywords_query), 1)\n",
    "        results.append({\n",
    "            \"chunk\": chunk_text,\n",
    "            \"page_number\": payload[\"page_number\"],\n",
    "            \"score\": result.score,\n",
    "            \"keyword_overlap\": keyword_overlap\n",
    "        })\n",
    "\n",
    "    return rerank_results(query, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b56cc18-7ca9-438f-8268-6dc8976341f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank results\n",
    "def rerank_results(query, results):\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    query_embedding = model.encode([query], normalize_embeddings=True)[0]\n",
    "\n",
    "    reranked = []\n",
    "    for result in results:\n",
    "        chunk_embedding = model.encode([result[\"chunk\"]], normalize_embeddings=True)[0]\n",
    "        cos_sim = np.dot(query_embedding, chunk_embedding)\n",
    "        final_score = 0.7 * cos_sim + 0.3 * result[\"keyword_overlap\"]\n",
    "\n",
    "        reranked.append({\n",
    "            \"chunk\": result[\"chunk\"],\n",
    "            \"page_number\": result[\"page_number\"],\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "\n",
    "    return sorted(reranked, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3afef7-70ac-48bf-8d06-de6905990596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full process\n",
    "pdf_path = \"alice.pdf\"\n",
    "text_with_pages = extract_text_with_pages(pdf_path)\n",
    "chunks = semantic_chunking(text_with_pages)\n",
    "embeddings = generate_embeddings(chunks)\n",
    "\n",
    "create_collection()\n",
    "store_chunks(chunks, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f231721-ffd4-4658-97b8-00e561debb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caf9f01a121421f8f7b0e4d37128c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eac85737604a4588cb53c6d75dd21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a74ffe6412484c85d208bc12190c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6b658cb7794bd3a507d33ed383b4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969409d9be1a485686fe002b99c21915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd1823b1a9549c3ba1e5b8ea9bf4a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07088c8d1e0e4c37b7e79ae537b5af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cbc8930682440f807620baa432a8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14cf5a923284d4f890f29e102b42671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffc8b55dd1f4fa0aeee0864a5e325a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e16c70a704b43ed8b70c261f95ec777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: (Page 70)\n",
      "began, in rather a complaining tone, “and they “It’s a friend of mine—a Cheshire Cat,” said\n",
      "all quarrel so dreadfully one can’t hear one’s-self Alice: “allow me to introduce it.”\n",
      "speak—and they don’t seem to have any rules “I don’t like the look of it at all,” said the\n",
      "in particular; at least, if there are, nobody King: “however, it may kiss my hand if it\n",
      "attends to them—and you’ve no idea how con- likes.”\n",
      "fusing it is all the things being alive; for in- “I’d rather not,” the Cat remarked.\n",
      "Final Score: 0.5542\n",
      "--------------------------------------------------\n",
      "Result 2: (Page 49)\n",
      "whether it was good manners for her to speak While she was trying to fix on one, the cook\n",
      "first, “why your cat grins like that?” took the cauldron of soup off the fire, and at\n",
      "“It’s a Cheshire cat,” said the Duchess, once set to work throwing everything within\n",
      "“and that’s why.\n",
      "Final Score: 0.5455\n",
      "--------------------------------------------------\n",
      "Result 3: (Page 52)\n",
      "89\n",
      "Alice was just beginning to think to herself, might do very well as pigs, and was just say-\n",
      "“Now, what am I to do with this creature ing to herself, “if one only knew the right way\n",
      "when I get it home?” when it grunted again, to change them——” when she was a little\n",
      "so violently, that she startled by seeing the Cheshire Cat sitting on\n",
      "looked down into its a bough of a tree a few yards off.\n",
      "Final Score: 0.4946\n",
      "--------------------------------------------------\n",
      "Result 4: (Page 49)\n",
      "The Duchess\n",
      "in another moment that it was addressed to took no notice of them even when they hit her;\n",
      "the baby, and not to her, so she took courage, and the baby was howling so much already, that\n",
      "and went on again:— it was quite impossible to say whether the\n",
      "“I didn’t know that Cheshire cats always blows hurt it or not.\n",
      "Final Score: 0.4446\n",
      "--------------------------------------------------\n",
      "Result 5: (Page 72)\n",
      "The Cat’s head began fading away the\n",
      "moment he was gone, and, by the time he had\n",
      "come back with the Duchess, it had entirely\n",
      "disappeared: so the King and the executioner\n",
      "ran wildly up and down looking for it, while\n",
      "the rest of the party went back to the game.\n",
      "Final Score: 0.4423\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Search example\n",
    "query = \"What is the Cheshire Cat's role in the story?\"\n",
    "results = search(query)\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"Result {i}: (Page {res['page_number']})\\n{res['chunk']}\\nFinal Score: {res['final_score']:.4f}\\n{'-'*50}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
